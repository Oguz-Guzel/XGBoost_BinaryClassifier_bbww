{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2574ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"numpy\",\n",
    "        \"scipy\",\n",
    "        \"matplotlib\",\n",
    "        \"mplhep\",\n",
    "        \"awkward\",\n",
    "        \"uproot\",\n",
    "        \"vector\",\n",
    "        \"xgboost\",\n",
    "    ],\n",
    ")\n",
    "def train(\n",
    "    bamboo_results_dir: list,\n",
    "    N: int,\n",
    "    output_dir: str,\n",
    "):\n",
    "    import os\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import mplhep as hep\n",
    "\n",
    "    hep.style.use(\"CMS\")\n",
    "    plt.rcParams[\"figure.dpi\"] = 400\n",
    "\n",
    "    import json\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "    N = int(N)  # Convert to integer for consistency\n",
    "    noise_level = 0.01\n",
    "\n",
    "\n",
    "    os.makedirs(f\"{output_dir}\", exist_ok=True)\n",
    "\n",
    "    batch_size = 1024\n",
    "\n",
    "    print(f\"Using {output_dir} as output directory\\n\")\n",
    "    print(f\"Batch size: {batch_size}\\n\")\n",
    "    if N > 1e6:\n",
    "        print(\"Taking all events\")\n",
    "        print(\"(N =\", N, \")\\n\")\n",
    "    else:\n",
    "        print(\"Taking N =\", N, \" events\\n\")\n",
    "\n",
    "\n",
    "    def listFiles(prefixes):\n",
    "        return [\n",
    "            file for file in rootFiles if any(file.split(\"/\")[-1].startswith(p) for p in prefixes)\n",
    "        ]\n",
    "\n",
    "    # get the data files\n",
    "    rootFiles = []\n",
    "    for path in bamboo_results_dir:\n",
    "        for file in os.listdir(path):\n",
    "            rootFiles.append(os.path.join(path, file))\n",
    "\n",
    "    TT_files = listFiles([\"TT\"])\n",
    "    DY_files = listFiles([\"DY\"])\n",
    "    other_bkg_files = listFiles([\"Tbar\", \"TW\", \"W\", \"Z\"])\n",
    "    HH_files = listFiles([\"ggH\"])\n",
    "    VBF_files = listFiles([\"VBF\"])\n",
    "\n",
    "    config_dict = {\n",
    "        \"output_dir\": output_dir,\n",
    "        \"bamboo_results_dir\": bamboo_results_dir,\n",
    "        \"n_events\": N,\n",
    "        \"noise_level\": noise_level,\n",
    "        \"root_files\": rootFiles,\n",
    "        \"TT_files\": TT_files,\n",
    "        \"DY_files\": DY_files,\n",
    "        \"other_bkg_files\": other_bkg_files,\n",
    "        \"HH_files\": HH_files,\n",
    "    }\n",
    "\n",
    "    with open(f\"{output_dir}/config.json\", \"w\") as f:\n",
    "        json.dump(config_dict, f, indent=4)\n",
    "    print(f\"Saved config to {output_dir}/config.json\")\n",
    "\n",
    "    print(\"Number of TT root files:\", len(TT_files))\n",
    "    print(\"Number of DY root files:\", len(DY_files))\n",
    "    print(\"Number of other_bkg root files:\", len(other_bkg_files))\n",
    "    print(\"Number of HH root files:\", len(HH_files))\n",
    "    print(\"Number of VBF root files:\", len(VBF_files))\n",
    "\n",
    "\n",
    "    treenames = [\n",
    "        \"DL_resolved_1b_ee_ml_vars;1\",\n",
    "        \"DL_resolved_1b_mumu_ml_vars;1\",\n",
    "        \"DL_resolved_1b_emu_ml_vars;1\",\n",
    "        \"DL_resolved_2b_ee_ml_vars;1\",\n",
    "        \"DL_resolved_2b_mumu_ml_vars;1\",\n",
    "        \"DL_resolved_2b_emu_ml_vars;1\",\n",
    "        \"DL_boosted_ee_ml_vars;1\",\n",
    "        \"DL_boosted_mumu_ml_vars;1\",\n",
    "        \"DL_boosted_emu_ml_vars;1\",\n",
    "    ]\n",
    "\n",
    "    # The variables for each object (here l1, l2, j1, j2, j3, j4, j8(ak8) and met)\n",
    "    input_features = {\n",
    "        \"lepton_1\": [\n",
    "            \"l1_Px\",\n",
    "            \"l1_Py\",\n",
    "            \"l1_Pz\",\n",
    "            \"l1_E\",\n",
    "            \"l1_pdgId\",\n",
    "            \"l1_charge\",\n",
    "            \"leading_lepton_pt\",\n",
    "        ],\n",
    "        \"lepton_2\": [\n",
    "            \"l2_Px\",\n",
    "            \"l2_Py\",\n",
    "            \"l2_Pz\",\n",
    "            \"l2_E\",\n",
    "            \"l2_pdgId\",\n",
    "            \"l2_charge\",\n",
    "            \"subleading_lepton_pt\",\n",
    "        ],\n",
    "        \"jet_1\": [\"j1_Px\", \"j1_Py\", \"j1_Pz\", \"j1_E\", \"j1_btag\"],\n",
    "        \"jet_2\": [\"j2_Px\", \"j2_Py\", \"j2_Pz\", \"j2_E\", \"j2_btag\"],\n",
    "        \"jet_3\": [\"j3_Px\", \"j3_Py\", \"j3_Pz\", \"j3_E\", \"j3_btag\"],\n",
    "        \"jet_4\": [\"j4_Px\", \"j4_Py\", \"j4_Pz\", \"j4_E\", \"j4_btag\"],\n",
    "        \"jet_8\": [\n",
    "            \"j8_Px\",\n",
    "            \"j8_Py\",\n",
    "            \"j8_Pz\",\n",
    "            \"j8_E\",\n",
    "            \"j8_btag\",\n",
    "            \"j8_tau1\",\n",
    "            \"j8_tau2\",\n",
    "            \"j8_tau3\",\n",
    "            \"j8_tau4\",\n",
    "            \"j8_msoftdrop\",\n",
    "        ],\n",
    "        \"met\": [\"met_Px\", \"met_Py\", \"met_E\"],  # , \"met_LD\", \"HT\"], include these\n",
    "        \"misc\": [\n",
    "            \"dR_l1_l2\",\n",
    "            \"dR_j1_j2\",\n",
    "            \"dR_dilepton_dijet\",\n",
    "            \"dR_dilepton_dibjet\",\n",
    "            \"abs_dphi_met_dilepton\",\n",
    "            \"min_dR_l1_ak4jets\",\n",
    "            \"min_dR_l2_ak4jets\",\n",
    "            \"min_dR_lead_bjet_leptons\",\n",
    "            \"min_dR_sublead_bjet_leptons\",\n",
    "            \"min_dR_ak4jets\",\n",
    "            \"min_abs_dphi_ak4jets\",\n",
    "            \"di_bjet_mass\",\n",
    "            \"di_lepton_mass\",\n",
    "            \"di_lepton_met_mass\",\n",
    "            \"VBF_tag\",\n",
    "            \"boosted_tag\",\n",
    "            \"run_year\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    n_features = sum([len(val) for val in input_features.values()])\n",
    "\n",
    "    print(\"\\nNumber of input features:\", n_features)\n",
    "    print(f\"which are: {[value for value in input_features.values()]}\\n\")\n",
    "\n",
    "    processes = [\"HH\", \"bkg\"]\n",
    "\n",
    "    target_set = [\n",
    "        \"HH\",\n",
    "    ]\n",
    "\n",
    "    import uproot\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Collect all input feature names into a flat list\n",
    "    feature_list = [item for sublist in input_features.values() for item in sublist]\n",
    "\n",
    "\n",
    "    class CustomData:\n",
    "        def __init__(\n",
    "            self,\n",
    "            files,\n",
    "            type,\n",
    "            treenames,\n",
    "            feature_list,\n",
    "            extra_cols=None,\n",
    "            n_events=N,\n",
    "            name=\"data\",\n",
    "        ):\n",
    "            self.files = files\n",
    "            self.type = type\n",
    "            self.treenames = treenames\n",
    "            self.feature_list = feature_list.copy()\n",
    "            if extra_cols:\n",
    "                self.feature_list += extra_cols\n",
    "            self.extra_cols = extra_cols\n",
    "            self.n_events = n_events\n",
    "            self.name = name\n",
    "\n",
    "            # Load the data from the files\n",
    "            self.df = self.load_data()\n",
    "\n",
    "            # Add one-hot encoded column based on type, and set as int32\n",
    "            if self.type.lower() == \"hh\":\n",
    "                self.df[\"HH\"] = np.int8(1)\n",
    "                self.df[\"bkg\"] = np.int8(0)\n",
    "            else:\n",
    "                self.df[\"HH\"] = np.int8(0)\n",
    "                self.df[\"bkg\"] = np.int8(1)\n",
    "\n",
    "        def load_data(self):\n",
    "            data_frames = []\n",
    "            for file in tqdm(self.files, desc=f\"Loading {self.name} data\"):\n",
    "                with uproot.open(file) as f:\n",
    "                    available_keys = f.keys()\n",
    "                    for treename in self.treenames:\n",
    "                        # Remove cycle number for matching\n",
    "                        base_treename = treename.split(\";\")[0]\n",
    "                        matched_key = None\n",
    "                        for key in available_keys:\n",
    "                            if base_treename in key:\n",
    "                                matched_key = key\n",
    "                                break\n",
    "                        if matched_key is not None:\n",
    "                            tree = f[matched_key]\n",
    "                            arrs = tree.arrays(\n",
    "                                self.feature_list, entry_stop=self.n_events, library=\"pd\"\n",
    "                            )\n",
    "                            data_frames.append(arrs)\n",
    "                        else:\n",
    "                            print(\n",
    "                                f\"Warning: Tree '{treename}' not found in {file.split('/')[-1]}. Skipping.\"\n",
    "                            )\n",
    "            if data_frames:\n",
    "                return pd.concat(data_frames, ignore_index=True)\n",
    "            else:\n",
    "                return pd.DataFrame()  # Return empty DataFrame if nothing found\n",
    "\n",
    "\n",
    "    extra_cols = [\"event_no\", \"weight\"]\n",
    "\n",
    "    DY_df = CustomData(\n",
    "        DY_files,\n",
    "        \"bkg\",\n",
    "        treenames,\n",
    "        feature_list,\n",
    "        extra_cols=extra_cols,\n",
    "        n_events=N,\n",
    "        name=\"DY\",\n",
    "    )\n",
    "\n",
    "    TT_df = CustomData(\n",
    "        TT_files,\n",
    "        \"bkg\",\n",
    "        treenames,\n",
    "        feature_list,\n",
    "        extra_cols=extra_cols,\n",
    "        n_events=N,\n",
    "        name=\"TT\",\n",
    "    )\n",
    "\n",
    "    other_bkg_df = CustomData(\n",
    "        other_bkg_files,\n",
    "        \"bkg\",\n",
    "        treenames,\n",
    "        feature_list,\n",
    "        extra_cols=extra_cols,\n",
    "        n_events=N,\n",
    "        name=\"Other_bkg\",\n",
    "    )\n",
    "\n",
    "    HH_df = CustomData(\n",
    "        HH_files,\n",
    "        \"HH\",\n",
    "        treenames,\n",
    "        feature_list,\n",
    "        extra_cols=extra_cols,\n",
    "        n_events=N,\n",
    "        name=\"ggF_HH\",\n",
    "    )\n",
    "\n",
    "    # Concatenate DataFrames from CustomData objects in customdata_set\n",
    "    customdata_set = [DY_df, TT_df, other_bkg_df, HH_df]\n",
    "    # customdata_set = [DY_df, HH_df]\n",
    "\n",
    "    for customdata in customdata_set:\n",
    "        print(customdata.df.shape[0], \" events in \", customdata.name)\n",
    "\n",
    "    dfs = [data.df for data in customdata_set]\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\nConcatenated DataFrame shape: {df.shape}\")\n",
    "\n",
    "    for col, type in df.dtypes.items():\n",
    "        print(f\"Column: {col}, Type: {type}\")\n",
    "\n",
    "    for col, type in df.dtypes.items():\n",
    "        print(f\"Column: {col}, Type: {type}\")\n",
    "\n",
    "    if \"event_no\" not in df.columns:\n",
    "        raise KeyError(\"The DataFrame does not contain 'event_no' column.\")\n",
    "\n",
    "    # Add target columns to the data in one-hot encoding\n",
    "    for process in processes:\n",
    "        for data in customdata_set:\n",
    "            if data.type == process:\n",
    "                data.df[process] = np.ones_like(data.df[\"event_no\"])\n",
    "            else:\n",
    "                data.df[process] = np.zeros_like(data.df[\"event_no\"])\n",
    "\n",
    "    weight_branch = \"weight\"\n",
    "\n",
    "    # Make pandas dataframes out of the data\n",
    "    # if N is less than the number of events in the root files, then here\n",
    "    # we're falling into `sample` which shuffles the data since we want\n",
    "    # to take a really random subset. But this approach may improve bias,\n",
    "    # instead stratification should be done or all events should be used.\n",
    "    # df = pd.concat(\n",
    "    #     [data.get_df().sample(n=min(N, len(data.get_df()))) for data in data_set]\n",
    "    # )\n",
    "\n",
    "    # convert tags to integers\n",
    "    df[\"VBF_tag\"] = df[\"VBF_tag\"].astype(np.int8)\n",
    "    df[\"boosted_tag\"] = df[\"boosted_tag\"].astype(np.int8)\n",
    "\n",
    "    # # one-hot encode pdgId and charge of leptons in separate columns\n",
    "    # for lep in [\"l1\", \"l2\"]:\n",
    "    #     # pdgId one-hot as integers\n",
    "    #     pdgid_dummies = pd.get_dummies(df[f\"{lep}_pdgId\"], prefix=f\"{lep}_pdgId\").astype(int)\n",
    "    #     df = pd.concat([df, pdgid_dummies], axis=1)\n",
    "    #     # charge one-hot as integers\n",
    "    #     charge_dummies = pd.get_dummies(df[f\"{lep}_charge\"], prefix=f\"{lep}_charge\").astype(int)\n",
    "    #     df = pd.concat([df, charge_dummies], axis=1)\n",
    "\n",
    "    print(\"Number of input features after one-hot encoding:\", n_features + 4 * 2)\n",
    "\n",
    "    assert df.shape[0] == len(\n",
    "        df[\"event_no\"]\n",
    "    ), \"Number of rows in the DataFrame does not match the number of event_no entries\"\n",
    "\n",
    "    # data cleaning\n",
    "\n",
    "    # Check for infinite or nan values\n",
    "    # Show a boolean DataFrame where inf values are True\n",
    "\n",
    "    # df.drop(\n",
    "    #     columns=[\"sample\", \"file\", \"tree\"], inplace=True\n",
    "    # )  # drop event_no column if exists\n",
    "    inf_mask = np.isinf(df.values)\n",
    "\n",
    "    # Get row and column indices of inf values\n",
    "    rows, cols = np.where(inf_mask)\n",
    "\n",
    "    for r, c in zip(rows, cols):\n",
    "        print(f\"Row: {df.index[r]}, Column: {df.columns[c]}, Value: {df.iloc[r, c]}\")\n",
    "\n",
    "    # Find rows with inf values\n",
    "    rows_with_inf = np.where(inf_mask)[0]\n",
    "    rows_with_inf = np.unique(rows_with_inf)  # Unique row indices\n",
    "\n",
    "    # Drop those rows\n",
    "    df = df.drop(df.index[rows_with_inf]).reset_index(drop=True)\n",
    "    print(f\"Dropped {len(rows_with_inf)} row(s) containing inf values.\")\n",
    "\n",
    "    # Check for rows where lepton pdgId is not ±11 or ±13\n",
    "    invalid_mask = ~(\n",
    "        df[\"l1_pdgId\"].abs().isin([11, 13]) &\n",
    "        df[\"l2_pdgId\"].abs().isin([11, 13])\n",
    "    )\n",
    "    invalid_rows = df[invalid_mask]\n",
    "    print(f\"Number of events with invalid lepton pdgId: {invalid_rows.shape[0]}\")\n",
    "    print(invalid_rows[[\"l1_pdgId\", \"l2_pdgId\"]])\n",
    "\n",
    "    # Keep only rows where both leptons have allowed pdgId values\n",
    "    allowed_pdgids = {-11, 11, -13, 13}\n",
    "    mask = df[\"l1_pdgId\"].isin(allowed_pdgids) & df[\"l2_pdgId\"].isin(allowed_pdgids)\n",
    "    print(\n",
    "        f\"Dropping {len(df) - mask.sum()} events with lepton pdgId not in {allowed_pdgids}\"\n",
    "    )\n",
    "    df = df[mask].reset_index(drop=True)\n",
    "\n",
    "    # Drop events where either lepton has a charge not equal to -1 or 1\n",
    "    allowed_charges = {-1, 1}\n",
    "    charge_mask = df[\"l1_charge\"].isin(allowed_charges) & df[\"l2_charge\"].isin(\n",
    "        allowed_charges\n",
    "    )\n",
    "    print(\n",
    "        f\"Dropping {len(df) - charge_mask.sum()} events with lepton charge not in {allowed_charges}\"\n",
    "    )\n",
    "    df = df[charge_mask].reset_index(drop=True)\n",
    "\n",
    "    from scipy.stats import zscore\n",
    "\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    z_scores = np.abs(zscore(numeric_df, nan_policy=\"omit\"))\n",
    "    threshold = 10\n",
    "    # experimentally z score of 10 gives around 1% of the outliers\n",
    "    outlier_mask = z_scores > threshold\n",
    "\n",
    "    outlier_rows = df[(outlier_mask).any(axis=1)]\n",
    "    print(f\"Number of rows with outliers (z-score > {threshold}): {outlier_rows.shape[0]}\")\n",
    "\n",
    "    # Drop outlier rows from the dataframe\n",
    "    df.drop(outlier_rows.index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(\"Outliers dropped.\")\n",
    "\n",
    "    # Need to cut out negative weights #\n",
    "    print(\n",
    "        f\"\\nTotal weight sum = {df[weight_branch].sum():1.3e}, with {(df[weight_branch]<0).sum()} negative weight events\"\n",
    "    )\n",
    "    df = df[df[weight_branch] > 0]\n",
    "    print(\n",
    "        f\"\\nAfter cutting out negative weights : total weight sum = {df[weight_branch].sum():1.3e}\"\n",
    "    )\n",
    "\n",
    "    # In case you restricted the number of events, need to rescale the weights\n",
    "    for process, data_process in zip(processes, customdata_set):\n",
    "        ratio = (\n",
    "            data_process.df[weight_branch].sum() / df[df[process] == 1][weight_branch].sum()\n",
    "        )\n",
    "        df.loc[df[process] == 1, weight_branch] *= ratio\n",
    "\n",
    "    # Plot the weights before normalisation\n",
    "    plt.hist(\n",
    "        df[df[\"HH\"] == 1][weight_branch],\n",
    "        label=\"Signal (ggF HH)\",\n",
    "        histtype=\"step\",\n",
    "        linewidth=1,\n",
    "        bins=100,\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        df[df[\"bkg\"] == 1][weight_branch],\n",
    "        label=\"Background\",\n",
    "        histtype=\"step\",\n",
    "        linewidth=1,\n",
    "        bins=100,\n",
    "        color=\"red\",\n",
    "    )\n",
    "    plt.xlabel(\"Event weight\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.title(\n",
    "        \"Event weights before normalisation\",\n",
    "        fontsize=24,\n",
    "        loc=\"left\",\n",
    "        pad=10,\n",
    "        fontproperties=\"Tex Gyre Heros:italic\",\n",
    "    )\n",
    "    plt.title(\n",
    "        \"(13.6 TeV)\", fontsize=24, loc=\"right\", pad=10, fontproperties=\"Tex Gyre Heros\"\n",
    "    )\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig(f\"{output_dir}/event_weights.png\")\n",
    "    plt.clf()\n",
    "    print(f\"\\nWeights saved to {output_dir}/event_weights.png\")\n",
    "\n",
    "    # Now equalize the sum of weight\n",
    "    print(\"\\nBefore reweighting\")\n",
    "    for process in processes:\n",
    "        print(\n",
    "            f\"{process} : N = {df[df[process]==1].shape[0]:6d}, sum(w) = {df[df[process]==1][weight_branch].sum():1.3e}\"\n",
    "        )\n",
    "\n",
    "    for process in processes:\n",
    "        df.loc[df[process] == 1, weight_branch] *= (\n",
    "            df.shape[0] / len(processes) / df[df[process] == 1][weight_branch].sum()\n",
    "        )\n",
    "\n",
    "    print(\"After reweighting\")\n",
    "    for process in processes:\n",
    "        print(\n",
    "            f\"{process} : N = {df[df[process]==1].shape[0]:6d}, sum(w) = {df[df[process]==1][weight_branch].sum():1.3e}\"\n",
    "        )\n",
    "\n",
    "    # # Increase the weight for HH events\n",
    "    # df.loc[df[\"HH\"] == 1, weight_branch] *= 5\n",
    "    # print(\"\\nAfter multiplying HH weights by 5\")\n",
    "    # for process in processes:\n",
    "    #     print(\n",
    "    #         f\"{process} : N = {df[df[process]==1].shape[0]:6d}, sum(w) = {df[df[process]==1][weight_branch].sum():1.3e}\"\n",
    "    #     )\n",
    "\n",
    "    # Plot the weights after normalisation\n",
    "    plt.hist(\n",
    "        df[df[\"HH\"] == 1][weight_branch],\n",
    "        label=\"Signal (ggF HH)\",\n",
    "        histtype=\"step\",\n",
    "        linewidth=1,\n",
    "        bins=100,\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        df[df[\"bkg\"] == 1][weight_branch],\n",
    "        label=\"Background\",\n",
    "        histtype=\"step\",\n",
    "        linewidth=1,\n",
    "        bins=100,\n",
    "        color=\"red\",\n",
    "    )\n",
    "    plt.xlabel(\"Event weight\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.title(\n",
    "        \"Event weights after normalisation\",\n",
    "        fontsize=24,\n",
    "        loc=\"left\",\n",
    "        pad=10,\n",
    "        fontproperties=\"Tex Gyre Heros:italic\",\n",
    "    )\n",
    "    plt.title(\n",
    "        \"(13.6 TeV)\", fontsize=24, loc=\"right\", pad=10, fontproperties=\"Tex Gyre Heros\"\n",
    "    )\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig(f\"{output_dir}/normalised_weights.png\")\n",
    "    plt.clf()\n",
    "    print(f\"\\nNormalised weights saved to {output_dir}/normalised_weights.png\")\n",
    "\n",
    "    # Convert all feature columns to float32 for efficiency\n",
    "    df[feature_list] = df[feature_list].astype(np.float32)\n",
    "    df[weight_branch] = df[weight_branch].astype(np.float32)\n",
    "    df[\"event_no\"] = df[\"event_no\"].astype(np.int64)\n",
    "\n",
    "    for col, type in df.dtypes.items():\n",
    "        print(col, \"    :\", type)\n",
    "\n",
    "    # # Create directory for histograms if it doesn't exist\n",
    "    # hist_dir = f\"{output_dir}/column_hists\"\n",
    "    # os.makedirs(hist_dir, exist_ok=True)\n",
    "\n",
    "    # # Plot each column as a histogram with linear and log y-scale side by side\n",
    "    # for col in df.columns:\n",
    "    #     fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    #     # Linear scale\n",
    "    #     axes[0].hist(df[col], bins=50, color=\"skyblue\")\n",
    "    #     axes[0].set_title(f\"{col} (linear)\")\n",
    "    #     axes[0].set_xlabel(col)\n",
    "    #     axes[0].set_ylabel(\"Count\")\n",
    "    #     # Log scale\n",
    "    #     axes[1].hist(df[col], bins=50, color=\"skyblue\")\n",
    "    #     axes[1].set_yscale(\"log\")\n",
    "    #     axes[1].set_title(f\"{col} (log)\")\n",
    "    #     axes[1].set_xlabel(col)\n",
    "    #     axes[1].set_ylabel(\"Count\")\n",
    "    #     plt.tight_layout()\n",
    "    #     fig.savefig(os.path.join(hist_dir, f\"{col}_hist.png\"))\n",
    "    #     plt.close(fig)\n",
    "    # print(f\"Saved all inputs as histograms to {hist_dir}\")\n",
    "\n",
    "    # if noise_level > 0.0:\n",
    "    #     # Inject some noise into the numeric columns\n",
    "    #     print(\"Adding noise.\")\n",
    "    #     # List of columns to augment (exclude targets and weights)\n",
    "    #     exclude_cols = [\"HH\", \"bkg\", \"weight\"]\n",
    "\n",
    "    #     numeric_cols = [\n",
    "    #         col\n",
    "    #         for col in df.select_dtypes(include=[np.number]).columns\n",
    "    #         if col not in exclude_cols\n",
    "    #     ]\n",
    "\n",
    "    #     # Set noise which is noise_level % of each column's std\n",
    "\n",
    "    #     # Create augmented dataframe\n",
    "    #     df_aug = df.copy()\n",
    "    #     for col in numeric_cols:\n",
    "    #         std = df[col].std()\n",
    "    #         noise = np.random.normal(0, noise_level * std, size=len(df))\n",
    "    #         df_aug[col] = df[col] + noise\n",
    "\n",
    "    #     # Concatenate augmented data to original\n",
    "    #     df = pd.concat([df, df_aug], ignore_index=True)\n",
    "\n",
    "    import xgboost as xgb\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "    # Prepare features and target\n",
    "    exclude_cols = [\"HH\", \"bkg\", \"weight\", \"event_no\"]\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    X = df[feature_cols].values\n",
    "    y = df[\"HH\"].values\n",
    "    weights = df[\"weight\"].values\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(\n",
    "        X, y, weights, test_size=0.2, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    # Train an XGBoost model\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.005,\n",
    "        max_depth=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"gpu\",\n",
    "        random_state=seed,\n",
    "        eval_metric=[\"logloss\", \"auc\"],\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        sample_weight=weights_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # obtain predictions\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    results = model.evals_result()\n",
    "\n",
    "    # Predict and evaluate\n",
    "    roc_auc = roc_auc_score(y_test, y_pred, sample_weight=weights_test)\n",
    "    print(f\"XGBoost ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred, sample_weight=weights_test)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"XGBoost ROC (AUC={roc_auc:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{output_dir}/xgb_roc_curve.png\")\n",
    "    plt.show()\n",
    "    print(f\"ROC curve saved to {output_dir}/xgb_roc_curve.png\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(results[\"validation_0\"][\"logloss\"], label=\"Training Logloss\", color=\"blue\", marker=\"o\", markersize=4, alpha=0.5)\n",
    "    plt.plot(results[\"validation_1\"][\"logloss\"], label=\"Validation Logloss\", color=\"red\", marker=\"s\", markersize=4, alpha=0.5)\n",
    "    plt.xlabel(\"Boosting Round\")\n",
    "    plt.ylabel(\"Logloss\")\n",
    "    plt.title(\"XGBoost Training and Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{output_dir}/xgb_loss_curve.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot score distribution for signal and background\n",
    "    plt.figure()\n",
    "    plt.hist(\n",
    "        y_pred[y_test == 1],\n",
    "        weights=weights_test[y_test == 1],\n",
    "        bins=50,\n",
    "        histtype=\"step\",\n",
    "        color=\"blue\",\n",
    "        label=\"Signal\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        y_pred[y_test == 0],\n",
    "        weights=weights_test[y_test == 0],\n",
    "        bins=50,\n",
    "        histtype=\"step\",\n",
    "        color=\"red\",\n",
    "        label=\"Background\",\n",
    "    )\n",
    "    plt.xlabel(\"XGBoost Output Score\")\n",
    "    plt.ylabel(\"Weighted Event Count\")\n",
    "    # plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(\"XGBoost Score Distribution\")\n",
    "    plt.savefig(f\"{output_dir}/xgb_score_dist.png\")\n",
    "    plt.show()\n",
    "    print(f\"Score distribution saved to {output_dir}/xgb_score_dist.png\")\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "    # Predict class labels for validation set\n",
    "    y_pred_label = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # Compute confusion matrix (normalized)\n",
    "    cm = confusion_matrix(\n",
    "        y_test, y_pred_label, sample_weight=weights_test, normalize=\"true\"\n",
    "    )\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm, display_labels=[\"Background\", \"Signal\"]\n",
    "    )\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\")\n",
    "    plt.title(\"Normalized Confusion Matrix (Validation)\")\n",
    "    plt.savefig(f\"{output_dir}/xgb_confusion_matrix.png\")\n",
    "    plt.show()\n",
    "    print(f\"Confusion matrix saved to {output_dir}/xgb_confusion_matrix.png\")\n",
    "\n",
    "    from xgboost import plot_importance\n",
    "\n",
    "    # Plot feature importance with feature names\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = plot_importance(\n",
    "        model,\n",
    "        max_num_features=len(feature_cols),\n",
    "        importance_type=\"gain\",\n",
    "        show_values=False,\n",
    "    )\n",
    "    feature_names = [col for col in df.columns if col not in [\"HH\", \"bkg\", \"weight\"]]\n",
    "    ax.set_yticklabels([feature_names[i] for i in range(len(ax.get_yticklabels()))])\n",
    "    plt.title(\"XGBoost Feature Importance\")\n",
    "    plt.tight_layout()\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, length=0)  # Remove y-axis ticks\n",
    "    plt.savefig(f\"{output_dir}/xgb_feature_importance.png\")\n",
    "    plt.show()\n",
    "    print(f\"Feature importance plot saved to {output_dir}/xgb_feature_importance.png\")\n",
    "\n",
    "    import onnxmltools\n",
    "    from onnxmltools.convert.common.data_types import FloatTensorType\n",
    "\n",
    "    onnx_model = onnxmltools.convert_xgboost(\n",
    "        model,\n",
    "        initial_types=[(\"float_input\", FloatTensorType([None, X_train.shape[1]]))],\n",
    "    )\n",
    "    onnx_path = f\"{output_dir}/model.onnx\"\n",
    "    with open(onnx_path, \"wb\") as f:\n",
    "        f.write(onnx_model.SerializeToString())\n",
    "    print(f\"XGBoost model exported to {onnx_path}\")\n",
    "\n",
    "    import onnxruntime as ort\n",
    "    import numpy as np\n",
    "\n",
    "    sess = ort.InferenceSession(onnx_path)\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    output_names = [o.name for o in sess.get_outputs()]\n",
    "\n",
    "    # Example input: use a real sample from your training data\n",
    "    signal_sample = df[df.HH == 1].sample(n=1000, random_state=seed)[feature_cols].values\n",
    "    bkg_sample = df[df.bkg == 1].sample(n=1000, random_state=seed)[feature_cols].values\n",
    "\n",
    "    signal_outputs = sess.run(output_names, {input_name: signal_sample.astype(np.float32)})\n",
    "    bkg_outputs = sess.run(output_names, {input_name: bkg_sample.astype(np.float32)})\n",
    "    signal_probs = signal_outputs[1]\n",
    "    bkg_probs = bkg_outputs[1]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(\n",
    "        signal_probs[:, 1],\n",
    "        bins=50,\n",
    "        color=\"skyblue\",\n",
    "        histtype=\"step\",\n",
    "        edgecolor=\"navy\",\n",
    "        label=\"signal\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        bkg_probs[:, 1],\n",
    "        bins=50,\n",
    "        color=\"salmon\",\n",
    "        histtype=\"step\",\n",
    "        edgecolor=\"red\",\n",
    "        label=\"background\",\n",
    "    )\n",
    "    plt.xlabel(\"Predicted Probability (class 1)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"ONNX Model Output Probabilities\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # import nbformat\n",
    "    # from nbconvert import PythonExporter\n",
    "\n",
    "    # notebook_path = \"XGBoost.ipynb\"\n",
    "    # script_path = \"XGBoost.py\"\n",
    "\n",
    "    # with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    #     nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "    # python_exporter = PythonExporter()\n",
    "    # script_body, _ = python_exporter.from_notebook_node(nb)\n",
    "\n",
    "    # with open(script_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    #     f.write(script_body)\n",
    "\n",
    "    # print(f\"Notebook converted to Python script: {script_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "422fea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@dsl.pipeline\n",
    "def training_pipeline(\n",
    "    bamboo_results_dir: list,\n",
    "    output_dir: str,\n",
    "    N: int,\n",
    ") -> None:\n",
    "    train_task = train(\n",
    "        bamboo_results_dir=bamboo_results_dir,\n",
    "        N=N,\n",
    "        output_dir=output_dir,\n",
    "    )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5cde7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f854eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=training_pipeline,\n",
    "    package_path='training_pipeline.yaml'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
