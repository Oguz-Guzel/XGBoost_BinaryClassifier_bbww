{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "270a5592-58e9-4c09-8039-a3e3cfb18ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q mplhep uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751f1b42-bc0e-46bb-ad72-7ac76248d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "\n",
    "hep.style.use(\"CMS\")\n",
    "plt.rcParams[\"figure.dpi\"] = 400\n",
    "\n",
    "import json\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa603fd3-d84d-42a2-ac60-6d9105bfe198",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"processed_data_v2\"\n",
    "bamboo_results_dir = [\n",
    "    \"/eos/user/a/aguzel/bamboo-output/v1.4.1-moreMLvars-2022/results/\",\n",
    "    \"/eos/user/a/aguzel/bamboo-output/v1.4.1-moreMLvars-2023/results/\",\n",
    "]\n",
    "N = 1e10\n",
    "N = int(N)  # Convert to integer for consistency\n",
    "noise_level = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73cbd27-d909-443a-9067-33ae041fd9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using processed_data_v2 as output directory\n",
      "\n",
      "Batch size: 1024\n",
      "\n",
      "Taking all events\n",
      "(N = 10000000000 )\n",
      "\n",
      "Saved config to processed_data_v2/config.json\n",
      "Number of TT root files: 4\n",
      "Number of DY root files: 8\n",
      "Number of other_bkg root files: 24\n",
      "Number of HH root files: 16\n",
      "Number of VBF root files: 4\n",
      "\n",
      "Number of input features: 68\n",
      "which are: [['l1_Px', 'l1_Py', 'l1_Pz', 'l1_E', 'l1_pdgId', 'l1_charge', 'leading_lepton_pt'], ['l2_Px', 'l2_Py', 'l2_Pz', 'l2_E', 'l2_pdgId', 'l2_charge', 'subleading_lepton_pt'], ['j1_Px', 'j1_Py', 'j1_Pz', 'j1_E', 'j1_btag'], ['j2_Px', 'j2_Py', 'j2_Pz', 'j2_E', 'j2_btag'], ['j3_Px', 'j3_Py', 'j3_Pz', 'j3_E', 'j3_btag'], ['j4_Px', 'j4_Py', 'j4_Pz', 'j4_E', 'j4_btag'], ['j8_Px', 'j8_Py', 'j8_Pz', 'j8_E', 'j8_btag', 'j8_tau1', 'j8_tau2', 'j8_tau3', 'j8_tau4', 'j8_msoftdrop'], ['met_Px', 'met_Py', 'met_E', 'HT', 'met_LD'], ['dR_l1_l2', 'dR_j1_j2', 'dR_dilepton_dijet', 'dR_dilepton_dibjet', 'abs_dphi_met_dilepton', 'abs_dphi_met_dibjet', 'min_dR_l1_ak4jets', 'min_dR_l2_ak4jets', 'min_dR_lead_bjet_leptons', 'min_dR_sublead_bjet_leptons', 'min_dR_ak4jets', 'min_abs_dphi_ak4jets', 'di_bjet_mass', 'di_lepton_mass', 'di_lepton_met_mass', 'di_lepton_dijet_met_mass', 'VBF_tag', 'boosted_tag', 'run_year']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "os.makedirs(f\"{output_dir}\", exist_ok=True)\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "print(f\"Using {output_dir} as output directory\\n\")\n",
    "print(f\"Batch size: {batch_size}\\n\")\n",
    "if N > 1e6:\n",
    "    print(\"Taking all events\")\n",
    "    print(\"(N =\", N, \")\\n\")\n",
    "else:\n",
    "    print(\"Taking N =\", N, \" events\\n\")\n",
    "\n",
    "\n",
    "def listFiles(prefixes):\n",
    "    return [\n",
    "        file\n",
    "        for file in rootFiles\n",
    "        if any(file.split(\"/\")[-1].startswith(p) for p in prefixes)\n",
    "    ]\n",
    "\n",
    "\n",
    "# get the data files\n",
    "rootFiles = []\n",
    "for path in bamboo_results_dir:\n",
    "    for file in os.listdir(path):\n",
    "        rootFiles.append(os.path.join(path, file))\n",
    "\n",
    "TT_files = listFiles([\"TT\"])\n",
    "DY_files = listFiles([\"DY\"])\n",
    "other_bkg_files = listFiles([\"Tbar\", \"TW\", \"W\", \"Z\"])\n",
    "HH_files = listFiles([\"ggH\"])\n",
    "VBF_files = listFiles([\"VBF\"])\n",
    "\n",
    "config_dict = {\n",
    "    \"output_dir\": output_dir,\n",
    "    \"bamboo_results_dir\": bamboo_results_dir,\n",
    "    \"n_events\": N,\n",
    "    \"noise_level\": noise_level,\n",
    "    \"root_files\": rootFiles,\n",
    "    \"TT_files\": TT_files,\n",
    "    \"DY_files\": DY_files,\n",
    "    \"other_bkg_files\": other_bkg_files,\n",
    "    \"HH_files\": HH_files,\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\", \"w\") as f:\n",
    "    json.dump(config_dict, f, indent=4)\n",
    "print(f\"Saved config to {output_dir}/config.json\")\n",
    "\n",
    "print(\"Number of TT root files:\", len(TT_files))\n",
    "print(\"Number of DY root files:\", len(DY_files))\n",
    "print(\"Number of other_bkg root files:\", len(other_bkg_files))\n",
    "print(\"Number of HH root files:\", len(HH_files))\n",
    "print(\"Number of VBF root files:\", len(VBF_files))\n",
    "\n",
    "treenames = [\n",
    "    \"DL_resolved_1b_ee_ml_vars;1\",\n",
    "    \"DL_resolved_1b_mumu_ml_vars;1\",\n",
    "    \"DL_resolved_1b_emu_ml_vars;1\",\n",
    "    \"DL_resolved_2b_ee_ml_vars;1\",\n",
    "    \"DL_resolved_2b_mumu_ml_vars;1\",\n",
    "    \"DL_resolved_2b_emu_ml_vars;1\",\n",
    "    \"DL_boosted_ee_ml_vars;1\",\n",
    "    \"DL_boosted_mumu_ml_vars;1\",\n",
    "    \"DL_boosted_emu_ml_vars;1\",\n",
    "]\n",
    "\n",
    "# The variables for each object (here l1, l2, j1, j2, j3, j4, j8(ak8) and met)\n",
    "input_features = {\n",
    "    \"lepton_1\": [\n",
    "        \"l1_Px\",\n",
    "        \"l1_Py\",\n",
    "        \"l1_Pz\",\n",
    "        \"l1_E\",\n",
    "        \"l1_pdgId\",\n",
    "        \"l1_charge\",\n",
    "        \"leading_lepton_pt\",\n",
    "    ],\n",
    "    \"lepton_2\": [\n",
    "        \"l2_Px\",\n",
    "        \"l2_Py\",\n",
    "        \"l2_Pz\",\n",
    "        \"l2_E\",\n",
    "        \"l2_pdgId\",\n",
    "        \"l2_charge\",\n",
    "        \"subleading_lepton_pt\",\n",
    "    ],\n",
    "    \"jet_1\": [\"j1_Px\", \"j1_Py\", \"j1_Pz\", \"j1_E\", \"j1_btag\"],\n",
    "    \"jet_2\": [\"j2_Px\", \"j2_Py\", \"j2_Pz\", \"j2_E\", \"j2_btag\"],\n",
    "    \"jet_3\": [\"j3_Px\", \"j3_Py\", \"j3_Pz\", \"j3_E\", \"j3_btag\"],\n",
    "    \"jet_4\": [\"j4_Px\", \"j4_Py\", \"j4_Pz\", \"j4_E\", \"j4_btag\"],\n",
    "    \"jet_8\": [\n",
    "        \"j8_Px\",\n",
    "        \"j8_Py\",\n",
    "        \"j8_Pz\",\n",
    "        \"j8_E\",\n",
    "        \"j8_btag\",\n",
    "        \"j8_tau1\",\n",
    "        \"j8_tau2\",\n",
    "        \"j8_tau3\",\n",
    "        \"j8_tau4\",\n",
    "        \"j8_msoftdrop\",\n",
    "    ],\n",
    "    \"met\": [\"met_Px\", \"met_Py\", \"met_E\", \"HT\", \"met_LD\"],\n",
    "    \"misc\": [\n",
    "        \"dR_l1_l2\",\n",
    "        \"dR_j1_j2\",\n",
    "        \"dR_dilepton_dijet\",\n",
    "        \"dR_dilepton_dibjet\",\n",
    "        \"abs_dphi_met_dilepton\",\n",
    "        \"abs_dphi_met_dibjet\",\n",
    "        \"min_dR_l1_ak4jets\",\n",
    "        \"min_dR_l2_ak4jets\",\n",
    "        \"min_dR_lead_bjet_leptons\",\n",
    "        \"min_dR_sublead_bjet_leptons\",\n",
    "        \"min_dR_ak4jets\",\n",
    "        \"min_abs_dphi_ak4jets\",\n",
    "        \"di_bjet_mass\",\n",
    "        \"di_lepton_mass\",\n",
    "        \"di_lepton_met_mass\",\n",
    "        \"di_lepton_dijet_met_mass\",\n",
    "        \"VBF_tag\",\n",
    "        \"boosted_tag\",\n",
    "        \"run_year\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "n_features = sum([len(val) for val in input_features.values()])\n",
    "\n",
    "print(\"\\nNumber of input features:\", n_features)\n",
    "print(f\"which are: {[value for value in input_features.values()]}\\n\")\n",
    "\n",
    "processes = [\"HH\", \"bkg\"]\n",
    "\n",
    "target_set = [\n",
    "    \"HH\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ff5a9e2-8bd8-48a1-bbce-0636e42aff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading DY data:  12%|█▎        | 1/8 [00:01<00:11,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tree 'DL_resolved_2b_emu_ml_vars;1' not found in DYto2L-2Jets_MLL-10to50_2022.root. Skipping.\n",
      "Warning: Tree 'DL_boosted_emu_ml_vars;1' not found in DYto2L-2Jets_MLL-10to50_2022.root. Skipping.\n",
      "Warning: Tree 'DL_resolved_2b_emu_ml_vars;1' not found in DYto2L-2Jets_MLL-10to50_2022EE.root. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading DY data:  62%|██████▎   | 5/8 [00:15<00:10,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tree 'DL_boosted_emu_ml_vars;1' not found in DYto2L-2Jets_MLL-10to50_2023.root. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading DY data:  75%|███████▌  | 6/8 [00:17<00:05,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tree 'DL_boosted_emu_ml_vars;1' not found in DYto2L-2Jets_MLL-10to50_2023BPix.root. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading DY data: 100%|██████████| 8/8 [00:22<00:00,  2.77s/it]\n",
      "Loading TT data: 100%|██████████| 4/4 [01:34<00:00, 23.68s/it]\n",
      "Loading Other_bkg data:  38%|███▊      | 9/24 [00:21<00:22,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tree 'DL_resolved_1b_ee_ml_vars;1' not found in WtoLNu-2Jets_2022.root. Skipping.\n",
      "Warning: Tree 'DL_resolved_2b_ee_ml_vars;1' not found in WtoLNu-2Jets_2022.root. Skipping.\n",
      "Warning: Tree 'DL_resolved_2b_mumu_ml_vars;1' not found in WtoLNu-2Jets_2022.root. Skipping.\n",
      "Warning: Tree 'DL_resolved_2b_emu_ml_vars;1' not found in WtoLNu-2Jets_2022.root. Skipping.\n",
      "Warning: Tree 'DL_boosted_ee_ml_vars;1' not found in WtoLNu-2Jets_2022.root. Skipping.\n",
      "Warning: Tree 'DL_boosted_mumu_ml_vars;1' not found in WtoLNu-2Jets_2022.root. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Other_bkg data:  42%|████▏     | 10/24 [00:22<00:15,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tree 'DL_resolved_1b_ee_ml_vars;1' not found in WtoLNu-2Jets_2022EE.root. Skipping.\n",
      "Warning: Tree 'DL_resolved_2b_ee_ml_vars;1' not found in WtoLNu-2Jets_2022EE.root. Skipping.\n",
      "Warning: Tree 'DL_resolved_2b_mumu_ml_vars;1' not found in WtoLNu-2Jets_2022EE.root. Skipping.\n",
      "Warning: Tree 'DL_resolved_2b_emu_ml_vars;1' not found in WtoLNu-2Jets_2022EE.root. Skipping.\n",
      "Warning: Tree 'DL_boosted_ee_ml_vars;1' not found in WtoLNu-2Jets_2022EE.root. Skipping.\n",
      "Warning: Tree 'DL_boosted_mumu_ml_vars;1' not found in WtoLNu-2Jets_2022EE.root. Skipping.\n",
      "Warning: Tree 'DL_boosted_emu_ml_vars;1' not found in WtoLNu-2Jets_2022EE.root. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Other_bkg data:  88%|████████▊ | 21/24 [00:45<00:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tree 'DL_resolved_2b_ee_ml_vars;1' not found in WtoLNu-2Jets_2023.root. Skipping.\n",
      "Warning: Tree 'DL_resolved_2b_mumu_ml_vars;1' not found in WtoLNu-2Jets_2023.root. Skipping.\n",
      "Warning: Tree 'DL_boosted_ee_ml_vars;1' not found in WtoLNu-2Jets_2023.root. Skipping.\n",
      "Warning: Tree 'DL_boosted_mumu_ml_vars;1' not found in WtoLNu-2Jets_2023.root. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Other_bkg data:  92%|█████████▏| 22/24 [00:45<00:02,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tree 'DL_resolved_1b_ee_ml_vars;1' not found in WtoLNu-2Jets_2023BPix.root. Skipping.\n",
      "Warning: Tree 'DL_resolved_2b_ee_ml_vars;1' not found in WtoLNu-2Jets_2023BPix.root. Skipping.\n",
      "Warning: Tree 'DL_resolved_2b_mumu_ml_vars;1' not found in WtoLNu-2Jets_2023BPix.root. Skipping.\n",
      "Warning: Tree 'DL_boosted_ee_ml_vars;1' not found in WtoLNu-2Jets_2023BPix.root. Skipping.\n",
      "Warning: Tree 'DL_boosted_mumu_ml_vars;1' not found in WtoLNu-2Jets_2023BPix.root. Skipping.\n",
      "Warning: Tree 'DL_boosted_emu_ml_vars;1' not found in WtoLNu-2Jets_2023BPix.root. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Other_bkg data: 100%|██████████| 24/24 [00:47<00:00,  1.98s/it]\n",
      "Loading ggF_HH data: 100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import uproot\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Collect all input feature names into a flat list\n",
    "feature_list = [item for sublist in input_features.values() for item in sublist]\n",
    "\n",
    "\n",
    "class CustomData:\n",
    "    def __init__(\n",
    "        self,\n",
    "        files,\n",
    "        type,\n",
    "        treenames,\n",
    "        feature_list,\n",
    "        extra_cols=None,\n",
    "        n_events=N,\n",
    "        name=\"data\",\n",
    "    ):\n",
    "        self.files = files\n",
    "        self.type = type\n",
    "        self.treenames = treenames\n",
    "        self.feature_list = feature_list.copy()\n",
    "        if extra_cols:\n",
    "            self.feature_list += extra_cols\n",
    "        self.extra_cols = extra_cols\n",
    "        self.n_events = n_events\n",
    "        self.name = name\n",
    "\n",
    "        # Load the data from the files\n",
    "        self.df = self.load_data()\n",
    "\n",
    "        # Add one-hot encoded column based on type, and set as int32\n",
    "        if self.type.lower() == \"hh\":\n",
    "            self.df[\"HH\"] = np.int8(1)\n",
    "            self.df[\"bkg\"] = np.int8(0)\n",
    "        else:\n",
    "            self.df[\"HH\"] = np.int8(0)\n",
    "            self.df[\"bkg\"] = np.int8(1)\n",
    "\n",
    "    def load_data(self):\n",
    "        data_frames = []\n",
    "        for file in tqdm(self.files, desc=f\"Loading {self.name} data\"):\n",
    "            with uproot.open(file) as f:\n",
    "                available_keys = f.keys()\n",
    "                for treename in self.treenames:\n",
    "                    # Remove cycle number for matching\n",
    "                    base_treename = treename.split(\";\")[0]\n",
    "                    matched_key = None\n",
    "                    for key in available_keys:\n",
    "                        if base_treename in key:\n",
    "                            matched_key = key\n",
    "                            break\n",
    "                    if matched_key is not None:\n",
    "                        tree = f[matched_key]\n",
    "                        arrs = tree.arrays(\n",
    "                            self.feature_list, entry_stop=self.n_events, library=\"pd\"\n",
    "                        )\n",
    "                        data_frames.append(arrs)\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"Warning: Tree '{treename}' not found in {file.split('/')[-1]}. Skipping.\"\n",
    "                        )\n",
    "        if data_frames:\n",
    "            return pd.concat(data_frames, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame()  # Return empty DataFrame if nothing found\n",
    "\n",
    "\n",
    "extra_cols = [\"event_no\", \"weight\"]\n",
    "\n",
    "DY_df = CustomData(\n",
    "    DY_files,\n",
    "    \"bkg\",\n",
    "    treenames,\n",
    "    feature_list,\n",
    "    extra_cols=extra_cols,\n",
    "    n_events=N,\n",
    "    name=\"DY\",\n",
    ")\n",
    "\n",
    "TT_df = CustomData(\n",
    "    TT_files,\n",
    "    \"bkg\",\n",
    "    treenames,\n",
    "    feature_list,\n",
    "    extra_cols=extra_cols,\n",
    "    n_events=N,\n",
    "    name=\"TT\",\n",
    ")\n",
    "\n",
    "other_bkg_df = CustomData(\n",
    "    other_bkg_files,\n",
    "    \"bkg\",\n",
    "    treenames,\n",
    "    feature_list,\n",
    "    extra_cols=extra_cols,\n",
    "    n_events=N,\n",
    "    name=\"Other_bkg\",\n",
    ")\n",
    "\n",
    "HH_df = CustomData(\n",
    "    HH_files,\n",
    "    \"HH\",\n",
    "    treenames,\n",
    "    feature_list,\n",
    "    extra_cols=extra_cols,\n",
    "    n_events=N,\n",
    "    name=\"ggF_HH\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b598f0e1-4ade-4844-bc97-f19e9b7d8ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106855  events in  DY\n",
      "16788207  events in  TT\n",
      "1901850  events in  Other_bkg\n",
      "314698  events in  ggF_HH\n",
      "\n",
      "Concatenated DataFrame shape: (19111610, 72)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Concatenate DataFrames from CustomData objects in customdata_set\n",
    "customdata_set = [DY_df, TT_df, other_bkg_df, HH_df]\n",
    "\n",
    "for customdata in customdata_set:\n",
    "    print(customdata.df.shape[0], \" events in \", customdata.name)\n",
    "\n",
    "dfs = [data.df for data in customdata_set]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"\\nConcatenated DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1eea95c-9ffe-46ea-95b6-ae170ab496e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: l1_Px, Type: float32\n",
      "Column: l1_Py, Type: float32\n",
      "Column: l1_Pz, Type: float32\n",
      "Column: l1_E, Type: float32\n",
      "Column: l1_pdgId, Type: float32\n",
      "Column: l1_charge, Type: float32\n",
      "Column: leading_lepton_pt, Type: float32\n",
      "Column: l2_Px, Type: float32\n",
      "Column: l2_Py, Type: float32\n",
      "Column: l2_Pz, Type: float32\n",
      "Column: l2_E, Type: float32\n",
      "Column: l2_pdgId, Type: float32\n",
      "Column: l2_charge, Type: float32\n",
      "Column: subleading_lepton_pt, Type: float32\n",
      "Column: j1_Px, Type: float32\n",
      "Column: j1_Py, Type: float32\n",
      "Column: j1_Pz, Type: float32\n",
      "Column: j1_E, Type: float32\n",
      "Column: j1_btag, Type: float64\n",
      "Column: j2_Px, Type: float32\n",
      "Column: j2_Py, Type: float32\n",
      "Column: j2_Pz, Type: float32\n",
      "Column: j2_E, Type: float32\n",
      "Column: j2_btag, Type: float64\n",
      "Column: j3_Px, Type: float32\n",
      "Column: j3_Py, Type: float32\n",
      "Column: j3_Pz, Type: float32\n",
      "Column: j3_E, Type: float32\n",
      "Column: j3_btag, Type: float64\n",
      "Column: j4_Px, Type: float32\n",
      "Column: j4_Py, Type: float32\n",
      "Column: j4_Pz, Type: float32\n",
      "Column: j4_E, Type: float32\n",
      "Column: j4_btag, Type: float64\n",
      "Column: j8_Px, Type: float32\n",
      "Column: j8_Py, Type: float32\n",
      "Column: j8_Pz, Type: float32\n",
      "Column: j8_E, Type: float32\n",
      "Column: j8_btag, Type: float64\n",
      "Column: j8_tau1, Type: float64\n",
      "Column: j8_tau2, Type: float64\n",
      "Column: j8_tau3, Type: float64\n",
      "Column: j8_tau4, Type: float64\n",
      "Column: j8_msoftdrop, Type: float64\n",
      "Column: met_Px, Type: float32\n",
      "Column: met_Py, Type: float32\n",
      "Column: met_E, Type: float32\n",
      "Column: HT, Type: float64\n",
      "Column: met_LD, Type: float64\n",
      "Column: dR_l1_l2, Type: float32\n",
      "Column: dR_j1_j2, Type: float32\n",
      "Column: dR_dilepton_dijet, Type: float64\n",
      "Column: dR_dilepton_dibjet, Type: float32\n",
      "Column: abs_dphi_met_dilepton, Type: float64\n",
      "Column: abs_dphi_met_dibjet, Type: float64\n",
      "Column: min_dR_l1_ak4jets, Type: float32\n",
      "Column: min_dR_l2_ak4jets, Type: float32\n",
      "Column: min_dR_lead_bjet_leptons, Type: float32\n",
      "Column: min_dR_sublead_bjet_leptons, Type: float32\n",
      "Column: min_dR_ak4jets, Type: float32\n",
      "Column: min_abs_dphi_ak4jets, Type: float64\n",
      "Column: di_bjet_mass, Type: float32\n",
      "Column: di_lepton_mass, Type: float32\n",
      "Column: di_lepton_met_mass, Type: float32\n",
      "Column: di_lepton_dijet_met_mass, Type: float32\n",
      "Column: VBF_tag, Type: bool\n",
      "Column: boosted_tag, Type: bool\n",
      "Column: run_year, Type: int32\n",
      "Column: event_no, Type: uint64\n",
      "Column: weight, Type: float64\n",
      "Column: HH, Type: int8\n",
      "Column: bkg, Type: int8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col, type in df.dtypes.items():\n",
    "    print(f\"Column: {col}, Type: {type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c300caae-0f11-413b-bcf7-42e3c1ad00ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"event_no\" not in df.columns:\n",
    "    raise KeyError(\"The DataFrame does not contain 'event_no' column.\")\n",
    "\n",
    "# Add target columns to the data in one-hot encoding\n",
    "for process in processes:\n",
    "    for data in customdata_set:\n",
    "        if data.type == process:\n",
    "            data.df[process] = np.ones_like(data.df[\"event_no\"])\n",
    "        else:\n",
    "            data.df[process] = np.zeros_like(data.df[\"event_no\"])\n",
    "\n",
    "weight_branch = \"weight\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edee3a24-613e-4cbc-bfd8-f9368334139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert tags to integers\n",
    "df[\"VBF_tag\"] = df[\"VBF_tag\"].astype(np.int8)\n",
    "df[\"boosted_tag\"] = df[\"boosted_tag\"].astype(np.int8)\n",
    "\n",
    "# # one-hot encode pdgId and charge of leptons in separate columns\n",
    "# for lep in [\"l1\", \"l2\"]:\n",
    "#     # pdgId one-hot as integers\n",
    "#     pdgid_dummies = pd.get_dummies(df[f\"{lep}_pdgId\"], prefix=f\"{lep}_pdgId\").astype(int)\n",
    "#     df = pd.concat([df, pdgid_dummies], axis=1)\n",
    "#     # charge one-hot as integers\n",
    "#     charge_dummies = pd.get_dummies(df[f\"{lep}_charge\"], prefix=f\"{lep}_charge\").astype(int)\n",
    "#     df = pd.concat([df, charge_dummies], axis=1)\n",
    "\n",
    "\n",
    "# print(\"Number of input features after one-hot encoding:\", n_features + 4 * 2)\n",
    "\n",
    "assert df.shape[0] == len(\n",
    "    df[\"event_no\"]\n",
    "), \"Number of rows in the DataFrame does not match the number of event_no entries\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04aad110-5477-4c71-ad3b-3d65c0b64f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: 7617044, Column: l2_Pz, Value: inf\n",
      "Row: 7617044, Column: l2_E, Value: inf\n",
      "Row: 17851717, Column: l2_Pz, Value: inf\n",
      "Row: 17851717, Column: l2_E, Value: inf\n",
      "Row: 18862599, Column: l2_Pz, Value: inf\n",
      "Row: 18862599, Column: l2_E, Value: inf\n",
      "Row: 18878321, Column: l2_Pz, Value: inf\n",
      "Row: 18878321, Column: l2_E, Value: inf\n",
      "Dropped 4 row(s) containing inf values.\n",
      "Dropping 1814 events with lepton pdgId not in {-11, 11, 13, -13}\n",
      "Dropping 0 events with lepton charge not in {1, -1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# data cleaning\n",
    "\n",
    "# Check for infinite or nan values\n",
    "# Show a boolean DataFrame where inf values are True\n",
    "\n",
    "# df.drop(\n",
    "#     columns=[\"sample\", \"file\", \"tree\"], inplace=True\n",
    "# )  # drop event_no column if exists\n",
    "inf_mask = np.isinf(df.values)\n",
    "\n",
    "# Get row and column indices of inf values\n",
    "rows, cols = np.where(inf_mask)\n",
    "\n",
    "for r, c in zip(rows, cols):\n",
    "    print(f\"Row: {df.index[r]}, Column: {df.columns[c]}, Value: {df.iloc[r, c]}\")\n",
    "\n",
    "# Find rows with inf values\n",
    "rows_with_inf = np.where(inf_mask)[0]\n",
    "rows_with_inf = np.unique(rows_with_inf)  # Unique row indices\n",
    "\n",
    "# Drop those rows\n",
    "df = df.drop(df.index[rows_with_inf]).reset_index(drop=True)\n",
    "print(f\"Dropped {len(rows_with_inf)} row(s) containing inf values.\")\n",
    "\n",
    "\n",
    "# Keep only rows where both leptons have allowed pdgId values\n",
    "allowed_pdgids = {-11, 11, -13, 13}\n",
    "mask = df[\"l1_pdgId\"].isin(allowed_pdgids) & df[\"l2_pdgId\"].isin(allowed_pdgids)\n",
    "print(\n",
    "    f\"Dropping {len(df) - mask.sum()} events with lepton pdgId not in {allowed_pdgids}\"\n",
    ")\n",
    "df = df[mask].reset_index(drop=True)\n",
    "\n",
    "# Drop events where either lepton has a charge not equal to -1 or 1\n",
    "allowed_charges = {-1, 1}\n",
    "charge_mask = df[\"l1_charge\"].isin(allowed_charges) & df[\"l2_charge\"].isin(\n",
    "    allowed_charges\n",
    ")\n",
    "print(\n",
    "    f\"Dropping {len(df) - charge_mask.sum()} events with lepton charge not in {allowed_charges}\"\n",
    ")\n",
    "df = df[charge_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03b35df5-9b67-44b1-b046-9d2758ab83c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with outliers (z-score > 10): 298680\n",
      "Outliers dropped.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "z_scores = np.abs(zscore(numeric_df, nan_policy=\"omit\"))\n",
    "threshold = 10\n",
    "# experimentally z score of 10 gives around 1% of the outliers\n",
    "outlier_mask = z_scores > threshold\n",
    "\n",
    "outlier_rows = df[(outlier_mask).any(axis=1)]\n",
    "print(f\"Number of rows with outliers (z-score > {threshold}): {outlier_rows.shape[0]}\")\n",
    "\n",
    "# Drop outlier rows from the dataframe\n",
    "df.drop(outlier_rows.index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"Outliers dropped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbf7cb47-db06-4f84-aa61-94c7ed01a942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total weight sum = 1.523e+09, with 71928 negative weight events\n",
      "\n",
      "After cutting out negative weights : total weight sum = 1.673e+09\n",
      "\n",
      "Weights saved to processed_data_v2/event_weights.png\n",
      "\n",
      "Before reweighting\n",
      "HH : N = 278724, sum(w) = 1.256e+09\n",
      "bkg : N = 18460460, sum(w) = 1.361e+09\n",
      "After reweighting\n",
      "HH : N = 278724, sum(w) = 9.370e+06\n",
      "bkg : N = 18460460, sum(w) = 9.370e+06\n",
      "\n",
      "Normalised weights saved to processed_data_v2/normalised_weights.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 4000x4000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Need to cut out negative weights #\n",
    "print(\n",
    "    f\"\\nTotal weight sum = {df[weight_branch].sum():1.3e}, with {(df[weight_branch]<0).sum()} negative weight events\"\n",
    ")\n",
    "df = df[df[weight_branch] > 0]\n",
    "print(\n",
    "    f\"\\nAfter cutting out negative weights : total weight sum = {df[weight_branch].sum():1.3e}\"\n",
    ")\n",
    "\n",
    "\n",
    "# In case you restricted the number of events, need to rescale the weights\n",
    "for process, data_process in zip(processes, customdata_set):\n",
    "    ratio = (\n",
    "        data_process.df[weight_branch].sum() / df[df[process] == 1][weight_branch].sum()\n",
    "    )\n",
    "    df.loc[df[process] == 1, weight_branch] *= ratio\n",
    "\n",
    "\n",
    "# Plot the weights before normalisation\n",
    "plt.hist(\n",
    "    df[df[\"HH\"] == 1][weight_branch],\n",
    "    label=\"Signal (ggF HH)\",\n",
    "    histtype=\"step\",\n",
    "    linewidth=1,\n",
    "    bins=100,\n",
    "    color=\"blue\",\n",
    ")\n",
    "plt.hist(\n",
    "    df[df[\"bkg\"] == 1][weight_branch],\n",
    "    label=\"Background\",\n",
    "    histtype=\"step\",\n",
    "    linewidth=1,\n",
    "    bins=100,\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.xlabel(\"Event weight\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\n",
    "    \"Event weights before normalisation\",\n",
    "    fontsize=24,\n",
    "    loc=\"left\",\n",
    "    pad=10,\n",
    "    fontproperties=\"Tex Gyre Heros:italic\",\n",
    ")\n",
    "plt.title(\n",
    "    \"(13.6 TeV)\", fontsize=24, loc=\"right\", pad=10, fontproperties=\"Tex Gyre Heros\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(f\"{output_dir}/event_weights.png\")\n",
    "plt.clf()\n",
    "print(f\"\\nWeights saved to {output_dir}/event_weights.png\")\n",
    "\n",
    "# Now equalize the sum of weight\n",
    "print(\"\\nBefore reweighting\")\n",
    "for process in processes:\n",
    "    print(\n",
    "        f\"{process} : N = {df[df[process]==1].shape[0]:6d}, sum(w) = {df[df[process]==1][weight_branch].sum():1.3e}\"\n",
    "    )\n",
    "\n",
    "for process in processes:\n",
    "    df.loc[df[process] == 1, weight_branch] *= (\n",
    "        df.shape[0] / len(processes) / df[df[process] == 1][weight_branch].sum()\n",
    "    )\n",
    "\n",
    "print(\"After reweighting\")\n",
    "for process in processes:\n",
    "    print(\n",
    "        f\"{process} : N = {df[df[process]==1].shape[0]:6d}, sum(w) = {df[df[process]==1][weight_branch].sum():1.3e}\"\n",
    "    )\n",
    "\n",
    "# Plot the weights after normalisation\n",
    "plt.hist(\n",
    "    df[df[\"HH\"] == 1][weight_branch],\n",
    "    label=\"Signal (ggF HH)\",\n",
    "    histtype=\"step\",\n",
    "    linewidth=1,\n",
    "    bins=100,\n",
    "    color=\"blue\",\n",
    ")\n",
    "plt.hist(\n",
    "    df[df[\"bkg\"] == 1][weight_branch],\n",
    "    label=\"Background\",\n",
    "    histtype=\"step\",\n",
    "    linewidth=1,\n",
    "    bins=100,\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.xlabel(\"Event weight\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\n",
    "    \"Event weights after normalisation\",\n",
    "    fontsize=24,\n",
    "    loc=\"left\",\n",
    "    pad=10,\n",
    "    fontproperties=\"Tex Gyre Heros:italic\",\n",
    ")\n",
    "plt.title(\n",
    "    \"(13.6 TeV)\", fontsize=24, loc=\"right\", pad=10, fontproperties=\"Tex Gyre Heros\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(f\"{output_dir}/normalised_weights.png\")\n",
    "plt.clf()\n",
    "print(f\"\\nNormalised weights saved to {output_dir}/normalised_weights.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e03a255-a966-4fcd-95fe-891993b78ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1_Px     : float32\n",
      "l1_Py     : float32\n",
      "l1_Pz     : float32\n",
      "l1_E     : float32\n",
      "l1_pdgId     : float32\n",
      "l1_charge     : float32\n",
      "leading_lepton_pt     : float32\n",
      "l2_Px     : float32\n",
      "l2_Py     : float32\n",
      "l2_Pz     : float32\n",
      "l2_E     : float32\n",
      "l2_pdgId     : float32\n",
      "l2_charge     : float32\n",
      "subleading_lepton_pt     : float32\n",
      "j1_Px     : float32\n",
      "j1_Py     : float32\n",
      "j1_Pz     : float32\n",
      "j1_E     : float32\n",
      "j1_btag     : float32\n",
      "j2_Px     : float32\n",
      "j2_Py     : float32\n",
      "j2_Pz     : float32\n",
      "j2_E     : float32\n",
      "j2_btag     : float32\n",
      "j3_Px     : float32\n",
      "j3_Py     : float32\n",
      "j3_Pz     : float32\n",
      "j3_E     : float32\n",
      "j3_btag     : float32\n",
      "j4_Px     : float32\n",
      "j4_Py     : float32\n",
      "j4_Pz     : float32\n",
      "j4_E     : float32\n",
      "j4_btag     : float32\n",
      "j8_Px     : float32\n",
      "j8_Py     : float32\n",
      "j8_Pz     : float32\n",
      "j8_E     : float32\n",
      "j8_btag     : float32\n",
      "j8_tau1     : float32\n",
      "j8_tau2     : float32\n",
      "j8_tau3     : float32\n",
      "j8_tau4     : float32\n",
      "j8_msoftdrop     : float32\n",
      "met_Px     : float32\n",
      "met_Py     : float32\n",
      "met_E     : float32\n",
      "HT     : float32\n",
      "met_LD     : float32\n",
      "dR_l1_l2     : float32\n",
      "dR_j1_j2     : float32\n",
      "dR_dilepton_dijet     : float32\n",
      "dR_dilepton_dibjet     : float32\n",
      "abs_dphi_met_dilepton     : float32\n",
      "abs_dphi_met_dibjet     : float32\n",
      "min_dR_l1_ak4jets     : float32\n",
      "min_dR_l2_ak4jets     : float32\n",
      "min_dR_lead_bjet_leptons     : float32\n",
      "min_dR_sublead_bjet_leptons     : float32\n",
      "min_dR_ak4jets     : float32\n",
      "min_abs_dphi_ak4jets     : float32\n",
      "di_bjet_mass     : float32\n",
      "di_lepton_mass     : float32\n",
      "di_lepton_met_mass     : float32\n",
      "di_lepton_dijet_met_mass     : float32\n",
      "VBF_tag     : float32\n",
      "boosted_tag     : float32\n",
      "run_year     : float32\n",
      "event_no     : int64\n",
      "weight     : float32\n",
      "HH     : int8\n",
      "bkg     : int8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert all feature columns to float32 for efficiency\n",
    "df[feature_list] = df[feature_list].astype(np.float32)\n",
    "df[weight_branch] = df[weight_branch].astype(np.float32)\n",
    "df[\"event_no\"] = df[\"event_no\"].astype(np.int64)\n",
    "\n",
    "\n",
    "for col, type in df.dtypes.items():\n",
    "    print(col, \"    :\", type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b2d3294-e8b4-4d1a-8829-16b98f9c3c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DataFrame saved to processed_data_v2/processed_data_v2.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.to_parquet(f\"{output_dir}/processed_data_v2.parquet\")\n",
    "print(f\"Processed DataFrame saved to {output_dir}/processed_data_v2.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe5e46-9761-4e79-b3b5-7e9b4e27e079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
